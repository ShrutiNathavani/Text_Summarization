{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shruti Nathavani\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Shruti Nathavani\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "import gensim\n",
    "from gensim.summarization import summarize\n",
    "    # Instantiating the model and tokenizer with gpt-2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''Before jumping into the term “Data Analysis”, let’s discuss the term “Analysis”. Analysis in Layman’s language (Plain English) is a process of answering “How?” and “Why?”. For example, how was the growth of XYZ Company in the last quarter? Or why did the sales of XYZ Company drop last summer? So to answer those questions we take the data that we already have. Out of that, we filter out what we need. This filtered data is the final dataset of the larger chunk that we have already collected and that becomes the target of data analysis. Or sometimes we take multiple data sets and analyze them to find a pattern. For example, take summer sales data for three consecutive years. Finding out if that fall in sales last summer was because of any specific product that we were selling or if it was just a recurring problem. It’s all about looking for a pattern. We analyze things or events that have already happened in the past. Taking all this information, we can define Data Analysis as:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 227, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Or why did the sales of XYZ Company drop last summer?\n",
      "Or sometimes we take multiple data sets and analyze them to find a pattern.\n",
      "For example, take summer sales data for three consecutive years.\n",
      "Taking all this information, we can define Data Analysis as:\n"
     ]
    }
   ],
   "source": [
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs=tokenizer.batch_encode_plus([text],return_tensors='pt',max_length=512,truncation=True)\n",
    "summary_ids=model.generate(inputs['input_ids'],early_stopping=True)\n",
    "GPT_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "#print(GPT_summary)\n",
    "# Passing the text corpus to summarizer \n",
    "short_summary = summarize(text)\n",
    "#print(short_summary)\n",
    "# Summarization by ratio\n",
    "summary_by_ratio=summarize(text,ratio=0.5)\n",
    "#print(summary_by_ratio)\n",
    "# Summarization by word count\n",
    "summary_by_word_count=summarize(text,word_count=50)\n",
    "print(summary_by_word_count)\n",
    "#return summary_by_word_count,GPT_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
